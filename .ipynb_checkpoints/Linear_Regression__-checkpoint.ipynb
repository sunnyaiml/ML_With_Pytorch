{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4a7477-5bba-4a96-9ff5-8fe1bcd53ee3",
   "metadata": {},
   "source": [
    "# Understanding Linear Regression: A Foundational Machine Learning Model\n",
    "\n",
    "Linear regression is a fundamental statistical method used for predictive analysis. It allows us to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "**When do we use Linear Regression?**\n",
    "We apply linear regression when our goal is to predict the value of one variable based on another, especially when we hypothesize a linear relationship exists between them.\n",
    "\n",
    "**Example:** Predicting a person's weight based on their height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659c91b-93bb-4f7b-8d7a-a47fa70834d5",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90516e-cfc5-4da1-b44f-392c78668dab",
   "metadata": {},
   "source": [
    "# Key Components of a Linear Regression Model\n",
    "\n",
    "To build and understand a linear regression model, it's crucial to grasp its fundamental components:\n",
    "\n",
    "*   **1. Dependent Variable (Y):**\n",
    "    *   This is the variable we aim to predict or explain.\n",
    "    *   **Examples:** Weight, Crop Yield, Sales Revenue.\n",
    "\n",
    "*   **2. Independent Variable (X):**\n",
    "    *   Also known as the predictor or explanatory variable, this is the input used to predict Y.\n",
    "    *   **Examples:** Height, Weather Conditions, Market Demand.\n",
    "\n",
    "*   **3. Intercept (a):**\n",
    "    *   The value of the dependent variable (Y) when all independent variables (X) are zero. It represents the point where the regression line crosses the Y-axis.\n",
    "\n",
    "*   **4. Slope (b):**\n",
    "    *   This coefficient quantifies the change in the dependent variable (Y) for every one-unit increase in the independent variable (X). It indicates the steepness and direction of the regression line.\n",
    "\n",
    "*   **5. Error Term (ε):**\n",
    "    *   Represents the irreducible error or noise in the data. It accounts for the difference between the actual observed values and the values predicted by the model, acknowledging that real-world data is rarely perfectly linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06a21f-7c00-49ab-8fdf-83304efb4926",
   "metadata": {},
   "source": [
    "# The Linear Regression Equation\n",
    "\n",
    "The relationship between the dependent and independent variables in linear regression is mathematically expressed as:\n",
    "\n",
    "$$\n",
    "Y = a + bX + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "*   **Y:** The Dependent Variable (what we predict)\n",
    "*   **a:** The Intercept\n",
    "*   **b:** The Slope\n",
    "*   **X:** The Independent Variable (input)\n",
    "*   **ε:** The Error Term\n",
    "\n",
    "This equation represents a straight line. The core objective of linear regression is to find the optimal values for 'a' (intercept) and 'b' (slope) that allow this line to best fit the given data, minimizing the error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45cf96-fd47-47e9-a791-5155fc0c6a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13a6a58-86ae-4025-85d0-cedf606b5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303d576-280a-40f8-a090-8619c7ad16ff",
   "metadata": {},
   "source": [
    "# Data Preparation: Setting Up Our Training Dataset\n",
    "\n",
    "Before we can train our linear regression model, we need to prepare our input and target data. For this project, we'll use NumPy arrays for initial data handling, which is a common practice when working with datasets, especially when reading from CSV files. These arrays will then be converted into PyTorch tensors, which are essential for leveraging PyTorch's automatic differentiation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a541f9-976c-42a6-8319-6e415cb7a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Input (temp, rainfall, humidity)\n",
    "\n",
    "\n",
    "Inputs = np.array([\n",
    "                   [73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]\n",
    "                    ], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "937c7efc-e562-436a-92b7-cced93aeaf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (apples, oranges)\n",
    "\n",
    "Targets = np.array([\n",
    "                    [56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119] \n",
    "                    ], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f582ec-32b0-49a5-85b5-eac816e9d7e3",
   "metadata": {},
   "source": [
    "so we create numpy array beacuse that is typically how you work with traning data\n",
    "read CSV file as numpy arrays, to do some processing and then covert them to pytorch tensor....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64ac728-0b38-4bb9-be89-d991bbdb5bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# create input and output tensor\n",
    "\n",
    "\n",
    "inputs = torch.from_numpy(Inputs)\n",
    "targets = torch.from_numpy(Targets)\n",
    "\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440acd61-bab6-4f76-9902-08d6cb45f53f",
   "metadata": {},
   "source": [
    "#### now lets create Linear regression model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d31024-8370-43bd-8dee-203fe80ee22f",
   "metadata": {},
   "source": [
    "##### The weights and biases (w11, w12,... w23, b1 & b2) can also be represented as matrices, \n",
    "##### initialized as random values. \n",
    "##### The first row of w and the first element of b are used to predict the first target variable\n",
    "##### i.e. yield of apples, and similarly the second for oranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a2327-3699-483f-b258-115f30ea2848",
   "metadata": {},
   "source": [
    "\n",
    "# Building a Linear Regression Model from Scratch in PyTorch\n",
    "\n",
    "Now, let's construct our linear regression model. We'll define the weights and biases, which are the learnable parameters of our model.\n",
    "\n",
    "**Weights and Biases Initialization:**\n",
    "The weights (w) and biases (b) are initialized as random values. In our case, 'w' is a 2x3 matrix and 'b' is a vector of 2 values.\n",
    "*   The first row of 'w' and the first element of 'b' will be used to predict the first target variable (e.g., yield of apples).\n",
    "*   Similarly, the second row of 'w' and the second element of 'b' will predict the second target variable (e.g., yield of oranges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd20b47-5a8b-47d6-8a50-999e224e8581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0895,  1.8823,  0.0517],\n",
      "        [-0.4927, -1.2695,  0.1607]], requires_grad=True)\n",
      "tensor([0.6706, 0.4246], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# weights and biases\n",
    "\n",
    "    \n",
    "w = torch.randn(2, 3, requires_grad=True) # we created 2 by 3 matrix of random value\n",
    "b = torch.randn(2, requires_grad=True) # we created a vertex of 2 value random value\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da6079-8363-4fc3-b0bf-ca8822e171e9",
   "metadata": {},
   "source": [
    "#### We can define the model as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5c98b8-28b4-405d-b498-b0f88b66c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e24ea3-6b46-4310-90e5-e9a5ecf17d67",
   "metadata": {},
   "source": [
    "#### '@' represents matrix multiplication in PyTorch, \n",
    "#### '.t()' method returns the transpose of a tensor.\n",
    "\n",
    "The matrix obtained by passing the input data into the model is a set of predictions for the target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd70985-24ea-48b6-9b6a-48673d5dd40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 135.5386, -113.6866],\n",
      "        [ 177.7626, -145.8393],\n",
      "        [ 263.6803, -203.2306],\n",
      "        [  92.6485,  -98.4701],\n",
      "        [ 191.1623, -144.1923]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6368e496-3276-41d3-894e-a1bb53bf4291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# now we compare with Targets\n",
    "\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122fcff-5ec3-4bd9-88a2-f78b53e98c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9185da0-96a9-48bb-91a0-5cdb406f2349",
   "metadata": {},
   "source": [
    "#### 🚨 Observing the Initial Model Performance\n",
    "\n",
    "We can clearly see there’s a **significant difference** between the predicted values and the actual target variables.\n",
    "\n",
    "---\n",
    "\n",
    "This isn’t surprising — we’ve **initialized the model with random weights and biases**, so it hasn't yet learned any meaningful patterns from the data.  \n",
    "As a result:\n",
    "\n",
    "- Predictions are far off.\n",
    "- The error (loss) is relatively high.\n",
    "- Performance is poor, as expected at this stage.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 What’s Next?\n",
    "\n",
    "We can’t expect the model to perform well right out of the gate. Now begins the **optimization process** — where we'll train the model using gradient descent to iteratively **adjust the weights and biases**, minimizing the loss and improving accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0358e2-71b2-40ea-b03b-a8199a05fcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b8e793f-d9b7-4107-873f-f61cddf58944",
   "metadata": {},
   "source": [
    "### Loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3d68f-d175-4ca6-866d-4e09a6bfcbd1",
   "metadata": {},
   "source": [
    "### 📏 Evaluating Model Performance: Mean Squared Error (MSE)\n",
    "\n",
    "Before improving our model, we need a way to **measure how well it's performing**. One common method is to calculate the **Mean Squared Error (MSE)** between the predictions and the actual targets.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Steps to Calculate MSE\n",
    "\n",
    "1. **Calculate the difference** between the two matrices — predictions (`preds`) and actual values (`targets`).\n",
    "2. **Square each element** of the difference matrix to eliminate negative values.\n",
    "3. **Compute the average** of all the squared differences.\n",
    "4. The final result is a **single scalar value** known as the **Mean Squared Error (MSE)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Why MSE?\n",
    "\n",
    "- MSE gives us a **quantitative measure of error**.\n",
    "- A **higher value** indicates worse performance, while a **lower MSE** means the model’s predictions are close to the actual values.\n",
    "\n",
    "Let’s use this metric to monitor and guide our improvements! 🔧📉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bb7401-2063-41b6-8cf7-980362295a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss \n",
    "\n",
    "def mse(t1 , t2):\n",
    "    diff = t1-t2\n",
    "    return torch.sum(diff**2)/diff.numel() #numel method returns the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da5fd6b-4887-44e3-94c4-73145560d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34472.9297, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Compute loss\n",
    "\n",
    "loss= mse(preds, targets)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece63d10-95f3-4e7e-bc4c-831b11264b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.66887107832588\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(math.sqrt(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14b03e-6438-445a-89e0-190e830c5db7",
   "metadata": {},
   "source": [
    "# Interpreting the Root Mean Squared Error (RMSE)\n",
    "\n",
    "After calculating the Mean Squared Error (MSE), taking its square root gives us the **Root Mean Squared Error (RMSE)**. RMSE is particularly useful because it returns the error in the same units as the dependent variable, making it more interpretable.\n",
    "\n",
    "**Our Current RMSE:** Approximately **185.66 units**.\n",
    "\n",
    "**What does this mean?**\n",
    "*   On average, each prediction made by our current model differs from the actual target value by about **185.66 units**.\n",
    "*   Given that our actual target values typically range from 50 to 200, an RMSE of 185.66 indicates a **very large error**. This suggests that our model is currently struggling significantly to capture the underlying patterns in the data.\n",
    "\n",
    "**Key Insight:**\n",
    "A **lower RMSE** indicates a better-performing model, as its predictions are closer to the actual values. A high RMSE, like the one we're seeing, is a strong signal that the model requires substantial improvement through further training and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f50b1b-2b87-4654-b2e9-669c3e0781a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "781c85f9-e76d-480f-9524-dc652e72cfbb",
   "metadata": {},
   "source": [
    "# Computing Gradients: The Engine of Learning\n",
    "\n",
    "In PyTorch, a powerful feature is its ability to automatically compute the gradients (or derivatives) of the loss function with respect to the model's parameters (weights and biases). This is possible because we set `requires_grad=True` when initializing `w` and `b`.\n",
    "\n",
    "**Why are Gradients Important?**\n",
    "Gradients tell us the direction and magnitude of the steepest ascent of the loss function. In optimization, we want to move in the opposite direction (steepest descent) to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df2a0b90-9e49-4cfa-80c5-739e5cb65f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f3870ea-1c01-4f0f-b311-920b0b71a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  8097.6494,   8946.5645,   5357.9561],\n",
      "        [-19420.3555, -22035.0859, -13326.6934]])\n",
      "tensor([[ 0.0895,  1.8823,  0.0517],\n",
      "        [-0.4927, -1.2695,  0.1607]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Gradient For Weight\n",
    "print(w.grad)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eddf8e-cb2a-4c28-8e44-f459bc32e6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad6b1b1-8eef-48eb-a5ce-efbf7dad16ae",
   "metadata": {},
   "source": [
    "# Loss Function Behavior and Gradient Resetting in PyTorch\n",
    "\n",
    "The loss function in linear regression is a **quadratic function** of our weights and biases. Our primary objective is to find the specific set of weights and biases that minimize this loss function.\n",
    "\n",
    "**Why Reset Gradients?**\n",
    "After computing gradients for a given batch or epoch, it's crucial to **reset them to zero** before the next iteration. This is done by calling `param.grad.zero_()` or `optimizer.zero_grad()`.\n",
    "\n",
    "**Reasoning:**\n",
    "PyTorch accumulates gradients by default. If we don't reset them, the gradients from previous computations would add up, leading to incorrect updates of the weights and biases in subsequent optimization steps. Resetting ensures that each gradient calculation is independent and based solely on the current iteration's loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cb2eba4-34dc-4877-9c19-ee24a7dbe010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f2d35-dae1-4a4d-8beb-4cdd167a182b",
   "metadata": {},
   "source": [
    "## 🎯 Adjust Weights and Biases Using Gradient Descent\n",
    "\n",
    "We'll reduce the loss and improve our model using the **gradient descent optimization algorithm**, which consists of the following steps:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Gradient Descent Steps:\n",
    "\n",
    "1. **Generate predictions**  \n",
    "   Feed inputs through the model to obtain predicted values.\n",
    "\n",
    "2. **Calculate the loss**  \n",
    "   Measure how far off the predictions are from actual values using a loss function.\n",
    "\n",
    "3. **Compute gradients w.r.t. weights and biases**  \n",
    "   Use backpropagation to find how the loss changes with respect to each parameter.\n",
    "\n",
    "4. **Adjust the weights and biases**  \n",
    "   Update them by subtracting a small value proportional to the gradient (learning rate × gradient).\n",
    "\n",
    "5. **Reset gradients to zero**  \n",
    "   Clear the accumulated gradients before the next iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6764b9-29fb-42c8-9668-6ebce2919e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6be3927-77ef-49ea-81c6-a8e228bd27f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 135.5386, -113.6866],\n",
      "        [ 177.7626, -145.8393],\n",
      "        [ 263.6803, -203.2306],\n",
      "        [  92.6485,  -98.4701],\n",
      "        [ 191.1623, -144.1923]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37b8338-519d-4ca2-b635-a5622c8535d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34472.9297, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12ba9778-110d-4616-b53f-ec1427a92884",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "843120db-aaed-4a57-9004-9136ba7f9f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  8097.6494,   8946.5645,   5357.9561],\n",
      "        [-19420.3555, -22035.0859, -13326.6934]])\n",
      "tensor([  95.9585, -233.0838])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1383928-261e-4c45-a126-f9ada07e4e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  8097.6494,   8946.5645,   5357.9561],\n",
      "        [-19420.3555, -22035.0859, -13326.6934]])\n",
      "tensor([[ 0.0895,  1.8823,  0.0517],\n",
      "        [-0.4927, -1.2695,  0.1607]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "\n",
    "print(w.grad)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b76bd-2ef3-46ec-b525-4fe5e2824065",
   "metadata": {},
   "source": [
    "# Optimizing with Gradient Descent: Adjusting Weights and Biases\n",
    "\n",
    "To reduce the loss and enhance our model's performance, we employ the **Gradient Descent optimization algorithm**. This iterative process systematically adjusts the model's parameters.\n",
    "\n",
    "**The Iterative Steps of Gradient Descent:**\n",
    "\n",
    "1.  **Generate Predictions:**\n",
    "    *   Pass the input data through the current model to obtain predicted values.\n",
    "\n",
    "2.  **Calculate the Loss:**\n",
    "    *   Quantify the discrepancy between the generated predictions and the actual target values using a chosen loss function (e.g., MSE).\n",
    "\n",
    "3.  **Compute Gradients:**\n",
    "    *   Utilize backpropagation to calculate the gradients of the loss with respect to each weight and bias. These gradients indicate the direction and rate of change of the loss function.\n",
    "\n",
    "4.  **Adjust Weights and Biases:**\n",
    "    *   Update the weights and biases by subtracting a small value proportional to their respective gradients. This \"small value\" is determined by the `learning rate` multiplied by the gradient. This step moves the parameters in the direction that minimizes the loss.\n",
    "\n",
    "    $$\n",
    "    \\text{New Parameter} = \\text{Old Parameter} - (\\text{Learning Rate} \\times \\text{Gradient})\n",
    "    $$\n",
    "\n",
    "5.  **Reset Gradients to Zero:**\n",
    "    *   Clear the accumulated gradients from the current iteration. This prepares the model for the next iteration, preventing gradients from previous steps from influencing future updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b65f68f-f2a4-45af-9826-c37fd52d76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "713ce1b5-3724-4b24-af28-71aec19ad201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0085,  1.7928, -0.0019],\n",
      "        [-0.2985, -1.0492,  0.2940]], requires_grad=True)\n",
      "tensor([0.6697, 0.4270], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6afed121-b49e-4d97-b329-6ea10f357063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23413.2617, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e936c7-7d27-4882-a372-4e684aef613e",
   "metadata": {},
   "source": [
    "# Training for Multiple Epochs: Iterative Refinement\n",
    "\n",
    "To further reduce the loss and improve our model's accuracy, we repeat the process of adjusting weights and biases using gradients multiple times. Each complete pass through the entire training dataset is called an **epoch**.\n",
    "\n",
    "By training for a sufficient number of epochs, the model iteratively refines its parameters, gradually moving towards a state where the loss is minimized and predictions become more accurate. We will now train the model for 100 epochs to observe the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f009e80-64a0-4db0-8f46-bbebf54c64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f251d604-6d0d-40fc-a240-01fbc64966e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(189.8116, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc124ebc-2109-47e3-aead-da3fa73593ea",
   "metadata": {},
   "source": [
    "# Observing the Loss Reduction\n",
    "\n",
    "After training the model for 100 epochs, we can clearly see that the loss has significantly decreased compared to its initial value. This indicates that our Gradient Descent optimization process is effectively guiding the model to learn the underlying patterns in the data and make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de14e3-f147-40f9-aa93-394d4f29eb65",
   "metadata": {},
   "source": [
    "# Further Training and Improved Predictions\n",
    "\n",
    "Encouraged by the initial loss reduction, we continue training for an additional 50 epochs. As expected, the loss continues to decrease, demonstrating the iterative nature of gradient descent in converging towards an optimal solution.\n",
    "\n",
    "**Current State:**\n",
    "The predictions generated by our model are now remarkably close to the actual target variables. This signifies a well-trained model that has successfully learned the relationships within the data.\n",
    "\n",
    "**Next Steps:**\n",
    "We could achieve even better results by training for a few more epochs, potentially fine-tuning the learning rate, or exploring more advanced optimization techniques. This iterative improvement is a core aspect of machine learning model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ceddb1e-08fa-4b24-9f35-2571856e5c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.9992,  75.5538],\n",
       "        [ 74.7930, 105.3095],\n",
       "        [135.8236, 113.9767],\n",
       "        [ 18.7340,  67.1863],\n",
       "        [ 90.5238, 109.6879]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7ac549f-c17a-4838-8b6a-e8dd9d044426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ff761e2-c89d-4e35-9ee7-a11ca82dfad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bd71a76-8737-469e-8919-5c4fc93ea936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a1f28c5-65f0-410f-8fc1-d6b1c0d89db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(118.2886, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e750a7a-fc9a-433f-ac87-b596968c16fb",
   "metadata": {},
   "source": [
    "it's getting lower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66f1e75d-772a-4879-8076-a095cc473c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2467,  74.1533],\n",
       "        [ 75.3032, 104.1961],\n",
       "        [134.2652, 118.7605],\n",
       "        [ 20.2069,  59.0304],\n",
       "        [ 90.5520, 112.5102]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e945e-5677-4409-9792-a021ce7e1059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "989660ce-c148-4493-9a37-96277cd7da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b1332a-ec31-4be3-a8a1-dfe07dacaed0",
   "metadata": {},
   "source": [
    "### The prediction are now quite close to the target variables, and we can get even better results by training for a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "23719c3b-183b-49f1-94ac-661d8caa87a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNjUlEQVR4nO3deVxUVf8H8M9lQEBWcWERBFTc0lDUlNTUoDRNRdzFRCvN0sQtl18PpuZSluZSiZa5FGmmaEaaEW5oLrhmakiGigjugIgKzpzfH/NwH0cWGZgFrp/36zUvnXPP3PnOgZqP5557rySEECAiIiJSKAtzF0BERERkTAw7REREpGgMO0RERKRoDDtERESkaAw7REREpGgMO0RERKRoDDtERESkaAw7REREpGgMO0RERKRoDDtETxlJkjBjxgxzl2F2nTp1QqdOneTnFy5cgCRJWL16tdlqetzjNZrKsGHD4OPjY/L3JTIWhh2icvjyyy8hSRLatGlT5n1cuXIFM2bMwIkTJwxXWAW3e/duSJIkP6ysrFC3bl0MHToU//77r7nL08sff/yBGTNmIDMz0+TvfezYMUiShP/85z/F9klOToYkSZgwYYIJKyOqWBh2iMohOjoaPj4+OHz4MP75558y7ePKlSuYOXPmUxV2CowdOxbffvstVqxYge7du+OHH35A69atceXKFZPX4u3tjXv37uG1117T63V//PEHZs6caZawExAQgEaNGmHdunXF9vn+++8BAEOGDDFVWUQVDsMOURmlpKTgjz/+wMKFC1GzZk1ER0ebu6RKp0OHDhgyZAiGDx+OpUuX4tNPP8WtW7ewZs2aYl9z9+5do9QiSRJsbGygUqmMsn9jCQsLw7///ouDBw8WuX3dunVo1KgRAgICTFwZUcXBsENURtHR0ahWrRq6d++Ovn37Fht2MjMzMX78ePj4+MDa2hqenp4YOnQobty4gd27d6N169YAgOHDh8uHdQrWjfj4+GDYsGGF9vn4Wo68vDxMnz4dLVu2hJOTE+zs7NChQwfs2rVL78919epVWFpaYubMmYW2JSUlQZIkfP755wCA/Px8zJw5E35+frCxsUH16tXRvn17xMXF6f2+APDiiy8C0AZJAJgxYwYkScKZM2cwePBgVKtWDe3bt5f7f/fdd2jZsiVsbW3h4uKCgQMHIjU1tdB+V6xYgXr16sHW1hbPPfccEhISCvUpbs3O33//jf79+6NmzZqwtbVFw4YN8f7778v1vffeewAAX19f+ed34cIFo9RYlLCwMAD/m8F51NGjR5GUlCT3+emnn9C9e3d4eHjA2toa9erVw4cffgi1Wl3iexQcdty9e7dOe0lj1rdvX7i4uMDGxgatWrXC1q1bdfoY+neHqCQMO0RlFB0djdDQUFSpUgWDBg1CcnIyEhMTdfrk5OSgQ4cOWLp0KV5++WUsXrwYo0aNwt9//43Lly+jcePGmDVrFgBg5MiR+Pbbb/Htt9/ihRde0KuW7OxsfP311+jUqRM+/vhjzJgxA9evX0eXLl30Pjzm6uqKjh07YsOGDYW2/fDDD1CpVOjXrx8A7Zf9zJkz0blzZ3z++ed4//33UadOHRw7dkyv9yxw/vx5AED16tV12vv164fc3FzMnTsXI0aMAADMmTMHQ4cOhZ+fHxYuXIhx48YhPj4eL7zwgs4hpZUrV+Ktt96Cm5sb5s+fj3bt2qFnz55FBo7H/fnnn2jTpg127tyJESNGYPHixQgJCcHPP/8MAAgNDcWgQYMAAJ999pn886tZs6bJavT19cXzzz+PDRs2FAotBQFo8ODBAIDVq1fD3t4eEyZMwOLFi9GyZUtMnz4dU6dOfeL7lNbp06fRtm1bnD17FlOnTsWCBQtgZ2eHkJAQbN68We5n6N8dohIJItLbkSNHBAARFxcnhBBCo9EIT09PERERodNv+vTpAoCIiYkptA+NRiOEECIxMVEAEKtWrSrUx9vbW4SHhxdq79ixo+jYsaP8/OHDh+LBgwc6fW7fvi1cXV3F66+/rtMOQHzwwQclfr7ly5cLAOLUqVM67U2aNBEvvvii/Nzf31907969xH0VZdeuXQKA+Oabb8T169fFlStXxC+//CJ8fHyEJEkiMTFRCCHEBx98IACIQYMG6bz+woULQqVSiTlz5ui0nzp1SlhaWsrteXl5olatWqJ58+Y647NixQoBQGcMU1JSCv0cXnjhBeHg4CAuXryo8z4FPzshhPjkk08EAJGSkmL0GovzxRdfCABix44dcptarRa1a9cWgYGBcltubm6h17711luiatWq4v79+3JbeHi48Pb2lp8X/Lx27dql89qixiwoKEg0a9ZMZ38ajUY8//zzws/PT24r6+8OUVlwZoeoDKKjo+Hq6orOnTsD0K73GDBgANavX6/zr+tNmzbB398fvXv3LrQPSZIMVo9KpUKVKlUAABqNBrdu3cLDhw/RqlWrMv1LOTQ0FJaWlvjhhx/ktr/++gtnzpzBgAED5DZnZ2ecPn0aycnJZar79ddfR82aNeHh4YHu3bvj7t27WLNmDVq1aqXTb9SoUTrPY2JioNFo0L9/f9y4cUN+uLm5wc/PTz58d+TIEVy7dg2jRo2SxwfQnlrt5ORUYm3Xr1/H3r178frrr6NOnTo620rzszNFjQUGDBgAKysrnUNZe/bsQVpamnwICwBsbW3lv9+5cwc3btxAhw4dkJubi7///rtU71WSW7duYefOnejfv7+8/xs3buDmzZvo0qULkpOTkZaWBqD8vztE+mDYIdKTWq3G+vXr0blzZ6SkpOCff/7BP//8gzZt2uDq1auIj4+X+54/fx5NmzY1SV1r1qzBs88+K69/qFmzJn755RdkZWXpva8aNWogKChI51DWDz/8AEtLS4SGhspts2bNQmZmJho0aIBmzZrhvffew59//lnq95k+fTri4uKwc+dO/Pnnn7hy5UqRZ0P5+vrqPE9OToYQAn5+fqhZs6bO4+zZs7h27RoA4OLFiwAAPz8/ndcXnOpekoJT4Mv68zNFjQWqV6+OLl26YPPmzbh//z4A7SEsS0tL9O/fX+53+vRp9O7dG05OTnB0dETNmjXls7TK8nvyuH/++QdCCERGRhb6zB988AEAyJ+7vL87RPqwNHcBRJXNzp07kZ6ejvXr12P9+vWFtkdHR+Pll182yHsVN4OgVqt1zhr67rvvMGzYMISEhOC9995DrVq1oFKpMG/ePHkdjL4GDhyI4cOH48SJE2jevDk2bNiAoKAg1KhRQ+7zwgsv4Pz58/jpp5/w22+/4euvv8Znn32GqKgovPnmm098j2bNmiE4OPiJ/R6dkQC0s1eSJGH79u1Fnj1lb29fik9oXKaucciQIYiNjUVsbCx69uyJTZs24eWXX5bXD2VmZqJjx45wdHTErFmzUK9ePdjY2ODYsWOYMmUKNBpNsfsu6ffwUQX7mDRpErp06VLka+rXrw+g/L87RPpg2CHSU3R0NGrVqoUvvvii0LaYmBhs3rwZUVFRsLW1Rb169fDXX3+VuL+SDolUq1atyOu3XLx4Uedf/Rs3bkTdunURExOjs7+Cf02XRUhICN566y35UNa5c+cwbdq0Qv1cXFwwfPhwDB8+HDk5OXjhhRcwY8YMo35h1atXD0II+Pr6okGDBsX28/b2BqCdZSk40wvQngmUkpICf3//Yl9bML5l/fmZosZH9ezZEw4ODvj+++9hZWWF27dv6xzC2r17N27evImYmBidBfAFZ76VpFq1agBQ6HexYFaqQMGYWVlZlSrEmuN3h55OPIxFpId79+4hJiYGr776Kvr27VvoMWbMGNy5c0c+zbZPnz44efKkzlkoBYQQAAA7OzsAhb9IAO0X5sGDB5GXlye3xcbGFjpLp2DmoGCfAHDo0CEcOHCgzJ/V2dkZXbp0wYYNG7B+/XpUqVIFISEhOn1u3ryp89ze3h7169fHgwcPyvy+pREaGgqVSoWZM2fqfGZAOwYFdbVq1Qo1a9ZEVFSUzhiuXr36iRcBrFmzJl544QV88803uHTpUqH3KFDcz88UNT7K1tYWvXv3xrZt27Bs2TLY2dmhV69e8vaifkfy8vLw5ZdfPnHf3t7eUKlU2Lt3r07746+tVasWOnXqhOXLlyM9Pb3Qfq5fvy7/3Vy/O/R04swOkR62bt2KO3fuoGfPnkVub9u2rXyBwQEDBuC9997Dxo0b0a9fP7z++uto2bIlbt26ha1btyIqKgr+/v6oV68enJ2dERUVBQcHB9jZ2aFNmzbw9fXFm2++iY0bN6Jr167o378/zp8/j++++w716tXTed9XX30VMTEx6N27N7p3746UlBRERUWhSZMmyMnJKfPnHTBgAIYMGYIvv/wSXbp0gbOzs872Jk2aoFOnTmjZsiVcXFxw5MgRbNy4EWPGjCnze5ZGvXr1MHv2bEybNg0XLlxASEgIHBwckJKSgs2bN2PkyJGYNGkSrKysMHv2bLz11lt48cUXMWDAAKSkpGDVqlWlWg+zZMkStG/fHgEBARg5ciR8fX1x4cIF/PLLL/Ip/S1btgQAvP/++xg4cCCsrKzQo0cPk9X4qCFDhmDt2rXYsWMHwsLC5CAGAM8//zyqVauG8PBwjB07FpIk4dtvvy0UxIri5OSEfv36YenSpZAkCfXq1UNsbKy8/uZRX3zxBdq3b49mzZphxIgRqFu3Lq5evYoDBw7g8uXLOHnyJADz/e7QU8ocp4ARVVY9evQQNjY24u7du8X2GTZsmLCyshI3btwQQghx8+ZNMWbMGFG7dm1RpUoV4enpKcLDw+XtQgjx008/iSZNmghLS8tCp/IuWLBA1K5dW1hbW4t27dqJI0eOFDr1XKPRiLlz5wpvb29hbW0tWrRoIWJjYwudQixE6U49L5CdnS1sbW0FAPHdd98V2j579mzx3HPPCWdnZ2FraysaNWok5syZI/Ly8krcb8GpzD/++GOJ/QpOPb9+/XqR2zdt2iTat28v7OzshJ2dnWjUqJEYPXq0SEpK0un35ZdfCl9fX2FtbS1atWol9u7dW2gMizqNWggh/vrrL9G7d2/h7OwsbGxsRMOGDUVkZKROnw8//FDUrl1bWFhYFDoN3ZA1PsnDhw+Fu7u7ACC2bdtWaPv+/ftF27Ztha2trfDw8BCTJ08WO3bsKHRaeVG/N9evXxd9+vQRVatWFdWqVRNvvfWW+Ouvv4ocs/Pnz4uhQ4cKNzc3YWVlJWrXri1effVVsXHjRrlPWX93iMpCEqIUsZ6IiIiokuKaHSIiIlI0hh0iIiJSNIYdIiIiUjSGHSIiIlI0hh0iIiJSNIYdIiIiUjReVBDa+7lcuXIFDg4OBr0TNRERERmPEAJ37tyBh4cHLCyKn79h2AFw5coVeHl5mbsMIiIiKoPU1FR4enoWu51hB4CDgwMA7WA5OjqauRoiIiIqjezsbHh5ecnf48Vh2MH/7lrs6OjIsENERFTJPGkJChcoExERkaIx7BAREZGiMewQERGRonHNTilpNBrk5eWZuwwqIysrK6hUKnOXQUREZsCwUwp5eXlISUmBRqMxdylUDs7OznBzc+O1lIiInjIMO08ghEB6ejpUKhW8vLxKvGgRVUxCCOTm5uLatWsAAHd3dzNXREREpsSw8wQPHz5Ebm4uPDw8ULVqVXOXQ2Vka2sLALh27Rpq1arFQ1pERE8RTlM8gVqtBgBUqVLFzJVQeRWE1fz8fDNXQkREpsSwU0pc51H58WdIRPR04mEsIiIiMgq1GkhIANLTAXd3oEMHwByrCDizQyYnSRK2bNli7jKIiMiIYmIAHx+gc2dg8GDtnz4+2nZTY9hRuAMHDkClUqF79+56vc7HxweLFi0yTlFERKRoMTFA377A5cu67Wlp2nZTBx6GHRNRq4Hdu4F167R//nfds9GtXLkS7777Lvbu3YsrV66Y5k2JiOippVYDERGAEIW3FbSNG2e670GAYcckzDWVl5OTgx9++AFvv/02unfvjtWrV+ts//nnn9G6dWvY2NigRo0a6N27NwCgU6dOuHjxIsaPHw9JkuSFvTNmzEDz5s119rFo0SL4+PjIzxMTE/HSSy+hRo0acHJyQseOHXHs2DFjfkwiIqpAEhIKz+g8SgggNVXbz1QYdozMnFN5GzZsQKNGjdCwYUMMGTIE33zzDcR/Y/Uvv/yC3r17o1u3bjh+/Dji4+Px3HPP/bfmGHh6emLWrFlIT09Henp6qd/zzp07CA8Px759+3Dw4EH4+fmhW7duuHPnjlE+IxERVSyl/crQ46ul3Hg2lhE9aSpPkrRTeb16GWd1+sqVKzFkyBAAQNeuXZGVlYU9e/agU6dOmDNnDgYOHIiZM2fK/f39/QEALi4uUKlUcHBwgJubm17v+eKLL+o8X7FiBZydnbFnzx68+uqr5fxERERU0ZX2IvWmvJg9Z3aMyJxTeUlJSTh8+DAGDRoEALC0tMSAAQOwcuVKAMCJEycQFBRk8Pe9evUqRowYAT8/Pzg5OcHR0RE5OTm4dOmSwd+LiIgqng4dAE9P7T/oiyJJgJeXtp+pcGbHiMw5lbdy5Uo8fPgQHh4ecpsQAtbW1vj888/l2yfow8LCQj4MVuDxqxGHh4fj5s2bWLx4Mby9vWFtbY3AwEDeMZ6I6CmhUgGLF2uXakiS7tGNggC0aJFpr7fDmR0jMtdU3sOHD7F27VosWLAAJ06ckB8nT56Eh4cH1q1bh2effRbx8fHF7qNKlSryrTIK1KxZExkZGTqB58SJEzp99u/fj7Fjx6Jbt2545plnYG1tjRs3bhj08xERUcUWGgps3AjUrq3b7umpbQ8NNW09nNkxooKpvLS0otftSJJ2u6Gn8mJjY3H79m288cYbcHJy0tnWp08frFy5Ep988gmCgoJQr149DBw4EA8fPsS2bdswZcoUANrr7OzduxcDBw6EtbU1atSogU6dOuH69euYP38++vbti19//RXbt2+Ho6OjvH8/Pz98++23aNWqFbKzs/Hee++VaRaJiIgqt9BQ7ZpUXkFZ4Qqm8oDCxy6NOZW3cuVKBAcHFwo6gDbsHDlyBC4uLvjxxx+xdetWNG/eHC+++CIOHz4s95s1axYuXLiAevXqoWbNmgCAxo0b48svv8QXX3wBf39/HD58GJMmTSr03rdv30ZAQABee+01jB07FrVq1TLsByQiokpBpQI6dQIGDdL+aY6gAwCSeHwRxlMoOzsbTk5OyMrK0pmlAID79+8jJSUFvr6+sLGxKdP+Y2K0Z2U9uljZy0sbdEw9lfc0M8TPkoiIKo6Svr8fxcNYJlCRpvKIiIieNgw7JlIwlUdERESmxTU7REREpGgMO0RERKRoDDtERESkaAw7REREpGgMO0RERKRoDDtERESkaAw7REREpGgMO1Quw4YNQ0hIiPy8U6dOGDdunMnr2L17NyRJQmZmpsnfm4iIKjaGHYUaNmwYJEmCJEmoUqUK6tevj1mzZuHhw4dGfd+YmBh8+OGHperLgEJERKbAKyibilpt8vtFdO3aFatWrcKDBw+wbds2jB49GlZWVpg2bZpOv7y8PFSpUsUg7+ni4mKQ/RARERkKZ3ZMISYG8PEBOncGBg/W/unjo203Imtra7i5ucHb2xtvv/02goODsXXrVvnQ05w5c+Dh4YGGDRsCAFJTU9G/f384OzvDxcUFvXr1woULF+T9qdVqTJgwAc7OzqhevTomT56Mx+8j+/hhrAcPHmDKlCnw8vKCtbU16tevj5UrV+LChQvo3LkzAKBatWqQJAnDhg0DAGg0GsybNw++vr6wtbWFv78/Nm7cqPM+27ZtQ4MGDWBra4vOnTvr1ElERPQohh1ji4kB+vbVveU5AKSladuNHHgeZWtri7y8PABAfHw8kpKSEBcXh9jYWOTn56NLly5wcHBAQkIC9u/fD3t7e3Tt2lV+zYIFC7B69Wp888032LdvH27duoXNmzeX+J5Dhw7FunXrsGTJEpw9exbLly+Hvb09vLy8sGnTJgBAUlIS0tPTsXjxYgDAvHnzsHbtWkRFReH06dMYP348hgwZgj179gDQhrLQ0FD06NEDJ06cwJtvvompU6caa9iIiKiS42EsY1KrgYgI4LHZDwDaNkkCxo3T3hLdiIe0hBCIj4/Hjh078O677+L69euws7PD119/LR+++u6776DRaPD1119DkiQAwKpVq+Ds7Izdu3fj5ZdfxqJFizBt2jSEhoYCAKKiorBjx45i3/fcuXPYsGED4uLiEBwcDACoW7euvL3gkFetWrXg7OwMQDsTNHfuXPz+++8IDAyUX7Nv3z4sX74cHTt2xLJly1CvXj0sWLAAANCwYUOcOnUKH3/8sQFHjYjoKWOG5RamwrBjTAkJhWd0HiUEkJqq7WeEW6LHxsbC3t4e+fn50Gg0GDx4MGbMmIHRo0ejWbNmOut0Tp48iX/++QcODg46+7h//z7Onz+PrKwspKeno02bNvI2S0tLtGrVqtChrAInTpyASqVCx44dS13zP//8g9zcXLz00ks67Xl5eWjRogUA4OzZszp1AJCDERERlUFMjPYf549+Z3l6AosXA//9B25lxrBjTOnphu2np86dO2PZsmWoUqUKPDw8YGn5vx+3nZ2dTt+cnBy0bNkS0dHRhfZTs2bNMr2/ra2t3q/JyckBAPzyyy+oXbu2zjZra+sy1UFERCUoWG7x+D9cC5ZbbNxY6QMP1+wYk7u7Yfvpyc7ODvXr10edOnV0gk5RAgICkJycjFq1aqF+/fo6DycnJzg5OcHd3R2HDh2SX/Pw4UMcPXq02H02a9YMGo1GXmvzuIKZJbVaLbc1adIE1tbWuHTpUqE6vLy8AACNGzfG4cOHdfZ18ODBkgeDiIgKe9JyC0C73OKR/09XRgw7xtShg3Ya8L9rYAqRJMDLS9vPzMLCwlCjRg306tULCQkJSElJwe7duzF27Fhc/u+0ZkREBD766CNs2bIFf//9N955550Sr5Hj4+OD8PBwvP7669iyZYu8zw0bNgAAvL29IUkSYmNjcf36deTk5MDBwQGTJk3C+PHjsWbNGpw/fx7Hjh3D0qVLsWbNGgDAqFGjkJycjPfeew9JSUn4/vvvsXr1amMPERGR8uiz3KISY9gxJpVKe7wTKBx4Cp4vWlQhFoBVrVoVe/fuRZ06dRAaGorGjRvjjTfewP379+Ho6AgAmDhxIl577TWEh4cjMDAQDg4O6N27d4n7XbZsGfr27Yt33nkHjRo1wogRI3D37l0AQO3atTFz5kxMnToVrq6uGDNmDADgww8/RGRkJObNm4fGjRuja9eu+OWXX+Dr6wsAqFOnDjZt2oQtW7bA398fUVFRmDt3rhFHh4hIocy83MJUJFHc6tKnSHZ2NpycnJCVlSV/sRe4f/8+UlJS4OvrCxsbm7K9QVELv7y8tEGnkh8HrUwM8rMkIlKS3bu11357kl27jHIiTXmV9P39KC5QNoXQUO3p5Qo9pY+IiCqpguUWaWlFr9uRJO32CrDcojwYdkxFpaqQqZiIiJ5iBcst+vbVBptHA08FW25RHlyzQ0RE9DQLDdWeXv7Y5T7g6amI084BzuwQERGRwpdbMOyUEtdxV378GRIRlUDByy14GOsJVP9NtQU3w6TKKzc3FwBgZWVl5kqIiMiUOLPzBJaWlqhatSquX78OKysrWFgwH1Y2Qgjk5ubi2rVrcHZ2lgMsERE9HRh2nkCSJLi7uyMlJQUXL140dzlUDs7OznBzczN3GUREZGIMO6VQpUoV+Pn58VBWJWZlZcUZHSKipxTDTilZWFjwqrtERESVEBegEBERkaIx7BAREZGimTXs7N27Fz169ICHhwckScKWLVvkbfn5+ZgyZQqaNWsGOzs7eHh4YOjQobhy5YrOPm7duoWwsDA4OjrC2dkZb7zxBnJyckz8SYiIiKiiMmvYuXv3Lvz9/fHFF18U2pabm4tjx44hMjISx44dQ0xMDJKSktCzZ0+dfmFhYTh9+jTi4uIQGxuLvXv3YuTIkab6CERERFTBSaKCXFZWkiRs3rwZISEhxfZJTEzEc889h4sXL6JOnTo4e/YsmjRpgsTERLRq1QoA8Ouvv6Jbt264fPkyPDw8SvXepb1FPBEREVUcpf3+rlRrdrKysiBJEpydnQEABw4cgLOzsxx0ACA4OBgWFhY4dOhQsft58OABsrOzdR5ERESlpVYDu3cD69Zp/1SrzV0RlaTShJ379+9jypQpGDRokJzeMjIyUKtWLZ1+lpaWcHFxQUZGRrH7mjdvHpycnOSHl5eXUWsnIiLliIkBfHyAzp2BwYO1f/r4aNupYqoUYSc/Px/9+/eHEALLli0r9/6mTZuGrKws+ZGammqAKomISOliYoC+fYHLl3Xb09K07Qw8FVOFDzsFQefixYuIi4vTOSbn5uaGa9eu6fR/+PAhbt26VeJtAaytreHo6KjzICIiKolaDUREAEWtdC1oGzeOh7QqogoddgqCTnJyMn7//XdUr15dZ3tgYCAyMzNx9OhRuW3nzp3QaDRo06aNqcslIiIFS0goPKPzKCGA1FRtP6pYzHq7iJycHPzzzz/y85SUFJw4cQIuLi5wd3dH3759cezYMcTGxkKtVsvrcFxcXFClShU0btwYXbt2xYgRIxAVFYX8/HyMGTMGAwcOLPWZWERERKWRnm7YfmQ6Zg07R44cQefOneXnEyZMAACEh4djxowZ2Lp1KwCgefPmOq/btWsXOnXqBACIjo7GmDFjEBQUBAsLC/Tp0wdLliwxSf1ERPT0cHc3bD8ynQpznR1z4nV2iIjoSdRq7VlXaWlFr9uRJMDTE0hJAVQqk5f3VFLkdXaIiIjMRaUCFi/W/l2SdLcVPF+0iEGnImLYISIiKqXQUGDjRqB2bd12T09te2ioeeqikpl1zQ4REVFlExoK9OqlPesqPV27RqdDB87oVGQMO0RERHpSqYD/nidDlQAPYxEREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRolmauwAiIqJKS60GEhKA9HTA3R3o0AFQqcxdFT2GYYeIiKgsYmKAiAjg8uX/tXl6AosXA6Gh5quLCuFhLCIiIn3FxAB9++oGHQBIS9O2x8SYpy4qEsMOERGRPtRq7YyOEIW3FbSNG6ftRxUCww4REZE+EhIKz+g8SgggNVXbjyoEhh0iIiJ9pKcbth8ZHcMOERGRPtzdDduPjI5hh4iISB8dOmjPupKkordLEuDlpe1HFQLDDhERkT5UKu3p5UDhwFPwfNEiXm+nAmHYISIi0ldoKLBxI1C7tm67p6e2ndfZqVB4UUEiIqKyCA0FevXiFZQrAYYdIiKislKpgE6dzF0FPQEPYxEREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiWZq7ACIiMiC1GkhIANLTAXd3oEMHQKUyd1VEZsWwQ0SkFDExQEQEcPny/9o8PYHFi4HQUPPVRWRmPIxFRKQEMTFA3766QQcA0tK07TEx5qmLqAJg2CEiqqDUamD3bmDdOu2fanUJHSMiACEKbytoGzeuhB0QKRvDDhFRBRQTA/j4AJ07A4MHa//08SlmgiYhofCMzqOEAFJTtf2InkIMO0REFYzeR6TS00u349L2I1IYhh0iogqkTEek3N1Lt/PS9iNSGLOGnb1796JHjx7w8PCAJEnYsmWLznYhBKZPnw53d3fY2toiODgYycnJOn1u3bqFsLAwODo6wtnZGW+88QZycnJM+CmIiAynTEekOnTQnnUlSUW/SJIALy9tP6KnkFnDzt27d+Hv748vvviiyO3z58/HkiVLEBUVhUOHDsHOzg5dunTB/fv35T5hYWE4ffo04uLiEBsbi71792LkyJGm+ghERAZVpiNSKpX29HKgcOApeL5oEa+3Q08vUUEAEJs3b5afazQa4ebmJj755BO5LTMzU1hbW4t169YJIYQ4c+aMACASExPlPtu3bxeSJIm0tLRSv3dWVpYAILKyssr/QYiIymHXLiG08zclP3btKuLFmzYJ4emp29HLS9tOpECl/f6usGt2UlJSkJGRgeDgYLnNyckJbdq0wYEDBwAABw4cgLOzM1q1aiX3CQ4OhoWFBQ4dOlTsvh88eIDs7GydBxFRRVCuI1KhocCFC8CuXcD332v/TEnhBQXpqVdhw05GRgYAwNXVVafd1dVV3paRkYFatWrpbLe0tISLi4vcpyjz5s2Dk5OT/PDy8jJw9UREZVPuI1IqFdCpEzBokPZPHroi0j/s3Lt3D7m5ufLzixcvYtGiRfjtt98MWpgxTZs2DVlZWfIjNTXV3CUREclCQ4GNG4HatXXbPT217ZyoIdKP3vfG6tWrF0JDQzFq1ChkZmaiTZs2sLKywo0bN7Bw4UK8/fbbBinMzc0NAHD16lW4P3K65NWrV9G8eXO5z7Vr13Re9/DhQ9y6dUt+fVGsra1hbW1tkDqJiIwhNBTo1Yv39CQyBL1ndo4dO4YO/z1YvHHjRri6uuLixYtYu3YtlixZYrDCfH194ebmhvj4eLktOzsbhw4dQmBgIAAgMDAQmZmZOHr0qNxn586d0Gg0aNOmjcFqISIyBx6RIjIMvWd2cnNz4eDgAAD47bffEBoaCgsLC7Rt2xYXL17Ua185OTn4559/5OcpKSk4ceIEXFxcUKdOHYwbNw6zZ8+Gn58ffH19ERkZCQ8PD4SEhAAAGjdujK5du2LEiBGIiopCfn4+xowZg4EDB8LDw0Pfj0ZEREQKpPfMTv369bFlyxakpqZix44dePnllwEA165dg6Ojo177OnLkCFq0aIEWLVoAACZMmIAWLVpg+vTpAIDJkyfj3XffxciRI9G6dWvk5OTg119/hY2NjbyP6OhoNGrUCEFBQejWrRvat2+PFStW6PuxiIiISKEkIYq6KHnxNm7ciMGDB0OtVuPFF19EXFwcAO0ZTnv37sX27duNUqgxZWdnw8nJCVlZWXoHNiIiIjKP0n5/6x12AO0p3+np6fD394eFhXZy6PDhw3B0dESjRo3KXrWZMOwQERFVPqX9/i7TdXbc3Nzg4OCAuLg43Lt3DwDQunXrShl0iIiISNn0Djs3b95EUFAQGjRogG7duiH9vzdoeeONNzBx4kSDF0hERERUHnqHnfHjx8PKygqXLl1C1apV5fYBAwbg119/NWhxREREROWl96nnv/32G3bs2AFPT0+ddj8/P71PPSciIiIyNr1ndu7evaszo1Pg1q1bvCoxERERVTh6h50OHTpg7dq18nNJkqDRaDB//nx07tzZoMURERERlZfeh7Hmz5+PoKAgHDlyBHl5eZg8eTJOnz6NW7duYf/+/caokYiIiKjM9J7Zadq0Kc6dO4f27dujV69euHv3LkJDQ3H8+HHUq1fPGDUSERERlVmZLiqoNLyoIBERUeVT2u9vvQ9j/fnnn0W2S5IEGxsb1KlThwuViYiIqMLQO+w0b94ckiQBAAomhQqeA4CVlRUGDBiA5cuX69ywk4iIiMgc9F6zs3nzZvj5+WHFihU4efIkTp48iRUrVqBhw4b4/vvvsXLlSuzcuRP/+c9/jFEvERERkV70ntmZM2cOFi9ejC5dushtzZo1g6enJyIjI3H48GHY2dlh4sSJ+PTTTw1aLBEREZG+9J7ZOXXqFLy9vQu1e3t749SpUwC0h7oK7plFREREZE56h51GjRrho48+Ql5entyWn5+Pjz76SL7reVpaGlxdXQ1XJREREVEZ6X0Y64svvkDPnj3h6emJZ599FoB2tketViM2NhYA8O+//+Kdd94xbKVEREREZVCm6+zcuXMH0dHROHfuHACgYcOGGDx4MBwcHAxeoCnwOjtERESVj9GuswMADg4OGDVqVJmLIyIiIjKVMoWd5ORk7Nq1C9euXYNGo9HZNn36dIMURkRERGQIeoedr776Cm+//TZq1KgBNzc3nQsKSpLEsENEREQVit5hZ/bs2ZgzZw6mTJlijHqIiIiIDErvU89v376Nfv36GaMWIiIiIoPTO+z069cPv/32mzFqISIiIjI4vQ9j1a9fH5GRkTh48CCaNWsGKysrne1jx441WHFERERE5aX3dXZ8fX2L35kk4d9//y13UabG6+wQERFVPka7zk5KSkq5CiMiIiIyJb3X7BARERFVJmW6qODly5exdetWXLp0SeeGoACwcOFCgxRGREREZAh6h534+Hj07NkTdevWxd9//42mTZviwoULEEIgICDAGDUSERERlZneh7GmTZuGSZMm4dSpU7CxscGmTZuQmpqKjh078vo7REREVOHoHXbOnj2LoUOHAgAsLS1x79492NvbY9asWfj4448NXiARERFReegdduzs7OR1Ou7u7jh//ry87caNG4arjIiIiMgA9F6z07ZtW+zbtw+NGzdGt27dMHHiRJw6dQoxMTFo27atMWokIiIiKjO9w87ChQuRk5MDAJg5cyZycnLwww8/wM/Pj2diERERUYWjV9hRq9W4fPkynn32WQDaQ1pRUVFGKYyIiIjIEPRas6NSqfDyyy/j9u3bxqqHiIiIyKD0XqDctGnTSnn/KyIiIno66R12Zs+ejUmTJiE2Nhbp6enIzs7WeRARERFVJHrf9dzC4n/5SJIk+e9CCEiSBLVabbjqTIR3PSciIqp8jHbX8127dpWrMCIiIiJT0jvsdOzY0Rh1EBERERmF3mEnMTER69atw7lz5wAADRs2xKBBg9CqVSuDF0dERERUXnotUJ48eTLatGmDr7/+GpcvX8bly5exYsUKtGnTBlOmTDFWjURERERlVuqws2bNGixduhRLlizBzZs3ceLECZw4cQK3bt3CZ599hiVLlmDt2rXGrJWIiIhIb6U+G+u5557DoEGDMH78+CK3L1y4EOvXr8fhw4cNWqAp8GwsIiKiyqe039+lntk5ffo0evXqVez2kJAQnD59Wr8qiYiIiIys1GFHpVIhLy+v2O35+flQqVQGKYqIiIjIUEoddgICAhAdHV3s9m+//RYBAQEGKYqIiIjIUEp96vmkSZMQEhKCBw8eYOLEiXB1dQUAZGRkYMGCBVi0aBE2b95stEKJiIiIykKv20UsXboUkyZNwsOHD+Hk5AQAyMrKgqWlJebPn4+IiAijFWpMXKBMRERU+ZT2+1vve2NdvnwZP/74I5KTkwEADRo0QJ8+feDl5VW+is2IYYeIiKjyMVrYUSKGHSIiosrH4KeeExEREVVGDDtERESkaAw7REREpGgMO0RERKRoeoedunXr4ubNm4XaMzMzUbduXYMURURERGQoeoedCxcuQK1WF2p/8OAB0tLSDFIUERERkaGU+grKW7dulf++Y8cO+aKCAKBWqxEfHw8fHx+DFqdWqzFjxgx89913yMjIgIeHB4YNG4b//Oc/kCQJACCEwAcffICvvvoKmZmZaNeuHZYtWwY/Pz+D1kJERESVU6nDTkhICABAkiSEh4frbLOysoKPjw8WLFhg0OI+/vhjLFu2DGvWrMEzzzyDI0eOYPjw4XBycsLYsWMBAPPnz8eSJUuwZs0a+Pr6IjIyEl26dMGZM2dgY2Nj0HqIiIio8tH7ooK+vr5ITExEjRo1jFWT7NVXX4WrqytWrlwpt/Xp0we2trb47rvvIISAh4cHJk6ciEmTJgHQ3r7C1dUVq1evxsCBA0v1PryoIBERUeVjtIsKpqSkyEHn/v37Za+wFJ5//nnEx8fj3LlzAICTJ09i3759eOWVV+RaMjIyEBwcLL/GyckJbdq0wYEDB4rd74MHD5Cdna3zICIiImXSO+xoNBp8+OGHqF27Nuzt7fHvv/8CACIjI3VmYAxh6tSpGDhwIBo1agQrKyu0aNEC48aNQ1hYGADtHdcByHdgL+Dq6ipvK8q8efPg5OQkPyrzfb2IiIioZHqHndmzZ2P16tWYP38+qlSpIrc3bdoUX3/9tUGL27BhA6Kjo/H999/j2LFjWLNmDT799FOsWbOmXPudNm0asrKy5EdqaqqBKiYiIqKKptQLlAusXbsWK1asQFBQEEaNGiW3+/v74++//zZoce+99548uwMAzZo1w8WLFzFv3jyEh4fDzc0NAHD16lW4u7vLr7t69SqaN29e7H6tra1hbW1t0FqJiIioYtJ7ZictLQ3169cv1K7RaJCfn2+Qogrk5ubCwkK3RJVKBY1GA0C7WNrNzQ3x8fHy9uzsbBw6dAiBgYEGrYWIiIgqJ71ndpo0aYKEhAR4e3vrtG/cuBEtWrQwWGEA0KNHD8yZMwd16tTBM888g+PHj2PhwoV4/fXXAWhPgx83bhxmz54NPz8/+dRzDw8P+VR5IiIierrpHXamT5+O8PBwpKWlQaPRICYmBklJSVi7di1iY2MNWtzSpUsRGRmJd955B9euXYOHhwfeeustTJ8+Xe4zefJk3L17FyNHjkRmZibat2+PX3/9ldfYISIiIgBluM4OACQkJGDWrFk4efIkcnJyEBAQgOnTp+Pll182Ro1Gx+vsEBERVT6l/f4uU9hRGoYdIiKiysdoFxUkIiIiqkz0XrNTrVo1+Sacj5IkCTY2Nqhfvz6GDRuG4cOHG6RAIiIiovIo0wLlOXPm4JVXXsFzzz0HADh8+DB+/fVXjB49GikpKXj77bfx8OFDjBgxwuAFExEREelD77Czb98+zJ49W+eCggCwfPly/Pbbb9i0aROeffZZLFmyhGGHiIiIzE7vNTs7duzQufFmgaCgIOzYsQMA0K1bN/meWURERETmpHfYcXFxwc8//1yo/eeff4aLiwsA4O7du3BwcCh/dURERETlpPdhrMjISLz99tvYtWuXvGYnMTER27ZtQ1RUFAAgLi4OHTt2NGylRERERGVQpuvs7N+/H59//jmSkpIAAA0bNsS7776L559/3uAFmgKvs0NERFT5lPb7W6+Znfz8fLz11luIjIzEunXryl0kERERkbHptWbHysoKmzZtMlYtRERERAan9wLlkJAQbNmyxQilEJFRqdXA7t3AunXaP9Vqc1dERGQSei9Q9vPzw6xZs7B//360bNkSdnZ2OtvHjh1rsOKIyEBiYoCICODy5f+1eXoCixcDoaHmq4uIyAT0XqDs6+tb/M4kqVJeX4cLlEnRYmKAvn2Bx/9TL7jty8aNDDxEVCnxrud6YNghxVKrAR8f3RmdR0mSdoYnJQVQqUxaGhFRefGu50QEJCQUH3QA7WxPaqq2HxGRQum9ZgcALl++jK1bt+LSpUvIy8vT2bZw4UKDFEZEBpCebth+RESVkN5hJz4+Hj179kTdunXx999/o2nTprhw4QKEEAgICDBGjURUVu7uhu1HRFQJ6X0Ya9q0aZg0aRJOnToFGxsbbNq0CampqejYsSP69etnjBqJqKw6dNCuySlYjPw4SQK8vLT9iIgUSu+wc/bsWQwdOhQAYGlpiXv37sHe3h6zZs3Cxx9/bPACiagcVCrt6eVA4cBT8HzRIi5OJiJF0zvs2NnZyet03N3dcf78eXnbjRs3DFcZERlGaKj29PLatXXbPT152jkRPRVKvWZn1qxZmDhxItq2bYt9+/ahcePG6NatGyZOnIhTp04hJiYGbdu2NWatRFRWoaFAr17as67S07VrdDp04IwOET0VSn2dHZVKhfT0dOTk5CAnJwfPPvss7t69i4kTJ+KPP/6An58fFi5cCG9vb2PXbHC8zg4REVHlY/C7nhdkorp168ptdnZ2iIqKKkeZRGRIajUnb4iIHqfXqedScWd0EJHZ8fZXRERF0yvsNGjQ4ImB59atW+UqiIj0V9ztr9LStO1ch0xETzO9ws7MmTPh5ORkrFqIqAzUau2MTlGr74TQnmE+bpx2fTIPaRHR00ivsDNw4EDUqlXLWLUQURnoc/urTp1MVhYRUYVR6uvscL0OUcXE218REZWs1GGnlGeoE5GJ8fZXREQlK/VhLI1GY8w6iKiMCm5/lZZW9LodSdJu5+2viOhppfftIoioYuHtr4iISsawQ6QAvP0VEVHx9Dobi4gqLt7+ioioaAw7RAqiUvH0ciKix/EwFhERESkaww4REREpGsMOERERKRrDDhERESkaww4REREpGsMOERERKRrDDhERESkaww4REREpGsMOERERKRrDDhERESkaww4REREpGsMOERERKRrDDhERESkaww4REREpGsMOERERKRrDDhERESkaww4REREpGsMOERERKRrDDhERESkaww4REREpGsMOERERKRrDDhERESkaww4REREpGsMOERERKVqFDztpaWkYMmQIqlevDltbWzRr1gxHjhyRtwshMH36dLi7u8PW1hbBwcFITk42Y8VERERUkVTosHP79m20a9cOVlZW2L59O86cOYMFCxagWrVqcp/58+djyZIliIqKwqFDh2BnZ4cuXbrg/v37ZqyciIiIKgpJCCHMXURxpk6div379yMhIaHI7UIIeHh4YOLEiZg0aRIAICsrC66urli9ejUGDhxYqvfJzs6Gk5MTsrKy4OjoaLD6iYiIyHhK+/1doWd2tm7dilatWqFfv36oVasWWrRoga+++krenpKSgoyMDAQHB8ttTk5OaNOmDQ4cOFDsfh88eIDs7GydBxERESlThQ47//77L5YtWwY/Pz/s2LEDb7/9NsaOHYs1a9YAADIyMgAArq6uOq9zdXWVtxVl3rx5cHJykh9eXl7G+xBERERkVhU67Gg0GgQEBGDu3Llo0aIFRo4ciREjRiAqKqpc+502bRqysrLkR2pqqoEqJiIiooqmQocdd3d3NGnSRKetcePGuHTpEgDAzc0NAHD16lWdPlevXpW3FcXa2hqOjo46DyIiIlKmCh122rVrh6SkJJ22c+fOwdvbGwDg6+sLNzc3xMfHy9uzs7Nx6NAhBAYGmrRWIiIiqpgszV1AScaPH4/nn38ec+fORf/+/XH48GGsWLECK1asAABIkoRx48Zh9uzZ8PPzg6+vLyIjI+Hh4YGQkBDzFk9EREQVQoUOO61bt8bmzZsxbdo0zJo1C76+vli0aBHCwsLkPpMnT8bdu3cxcuRIZGZmon379vj1119hY2NjxsqJiIiooqjQ19kxFV5nh4iIqPJRxHV2iIiIiMqLYYeIiIgUjWGHiIiIFI1hh4iIiBSNYYeIiIgUjWGHiIiIFK1CX2dHkdRqICEBSE8H3N2BDh0AlcrcVRERESkWw44pxcQAERHA5cv/a/P0BBYvBkJDzVcXERGRgvEwlqnExAB9++oGHQBIS9O2x8SYpy4iIiKFY9gxBbVaO6NT1MWqC9rGjdP2IyIiIoNi2DGFhITCMzqPEgJITdX2IyIiIoNi2DGF9HTD9iMiIqJSY9gxBXd3w/YjIiKiUmPYMYUOHbRnXUlS0dslCfDy0vYjIiIig2LYMQWVSnt6OVA48BQ8X7SI19shIiIyAoYdUwkNBTZuBGrX1m339NS28zo7RERERsGLCppSaCjQqxevoExERGRCDDumplIBnTqZuwoiIqKnBg9jERERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiVaqw89FHH0GSJIwbN05uu3//PkaPHo3q1avD3t4effr0wdWrV81XJBEREVUolSbsJCYmYvny5Xj22Wd12sePH4+ff/4ZP/74I/bs2YMrV64gNDTUTFUSERFRRVMpwk5OTg7CwsLw1VdfoVq1anJ7VlYWVq5ciYULF+LFF19Ey5YtsWrVKvzxxx84ePCgGSsmIiKiiqJShJ3Ro0eje/fuCA4O1mk/evQo8vPzddobNWqEOnXq4MCBA8Xu78GDB8jOztZ5EBERkTJZmruAJ1m/fj2OHTuGxMTEQtsyMjJQpUoVODs767S7uroiIyOj2H3OmzcPM2fONHSpREREVAFV6Jmd1NRUREREIDo6GjY2Ngbb77Rp05CVlSU/UlNTDbZvIiIiqlgqdNg5evQorl27hoCAAFhaWsLS0hJ79uzBkiVLYGlpCVdXV+Tl5SEzM1PndVevXoWbm1ux+7W2toajo6POg4iIiJSpQh/GCgoKwqlTp3Tahg8fjkaNGmHKlCnw8vKClZUV4uPj0adPHwBAUlISLl26hMDAQHOUTERERBVMhQ47Dg4OaNq0qU6bnZ0dqlevLre/8cYbmDBhAlxcXODo6Ih3330XgYGBaNu2rTlKJiIiogqmQoed0vjss89gYWGBPn364MGDB+jSpQu+/PJLc5dFREREFYQkhBDmLsLcsrOz4eTkhKysLIOt31GrgYQEID0dcHcHOnQAVCqD7JqIiIhQ+u/vSj+zUxHFxAAREcDly/9r8/QEFi8GeHFnIiIi06rQZ2NVRjExQN++ukEHANLStO0xMeapi4iI6GnFsGNAarV2RqeoA4MFbePGafsRERGRaTDsGFBCQuEZnUcJAaSmavsRERGRaTDsGFB6umH7ERERUfkx7BiQu7th+xEREVH5MewYUIcO2rOuJKno7ZIEeHlp+xEREZFpMOwYkEqlPb0cKBx4Cp4vWsTr7RAREZkSw46BhYYCGzcCtWvrtnt6att5nR0iIiLT4kUFjSA0FOjVi1dQJiIiqggYdoxEpQI6dTJ3FURERMTDWERERKRoDDtERESkaAw7REREpGgMO0RERKRoDDtERESkaAw7REREpGgMO0RERKRoDDtERESkaAw7REREpGi8gjIAIQQAIDs728yVEBERUWkVfG8XfI8Xh2EHwJ07dwAAXl5eZq6EiIiI9HXnzh04OTkVu10ST4pDTwGNRoMrV67AwcEBkiSV6jXZ2dnw8vJCamoqHB0djVwhARxzc+CYmx7H3PQ45qZnqDEXQuDOnTvw8PCAhUXxK3M4swPAwsICnp6eZXqto6Mj/+MwMY656XHMTY9jbnocc9MzxJiXNKNTgAuUiYiISNEYdoiIiEjRGHbKyNraGh988AGsra3NXcpTg2Nuehxz0+OYmx7H3PRMPeZcoExERESKxpkdIiIiUjSGHSIiIlI0hh0iIiJSNIYdIiIiUjSGnRLMmzcPrVu3hoODA2rVqoWQkBAkJSXp9Ll//z5Gjx6N6tWrw97eHn369MHVq1fNVLHyfPTRR5AkCePGjZPbOOaGl5aWhiFDhqB69eqwtbVFs2bNcOTIEXm7EALTp0+Hu7s7bG1tERwcjOTkZDNWXLmp1WpERkbC19cXtra2qFevHj788EOd+/twzMtn79696NGjBzw8PCBJErZs2aKzvTTje+vWLYSFhcHR0RHOzs544403kJOTY8JPUbmUNOb5+fmYMmUKmjVrBjs7O3h4eGDo0KG4cuWKzj6MNeYMOyXYs2cPRo8ejYMHDyIuLg75+fl4+eWXcffuXbnP+PHj8fPPP+PHH3/Enj17cOXKFYSGhpqxauVITEzE8uXL8eyzz+q0c8wN6/bt22jXrh2srKywfft2nDlzBgsWLEC1atXkPvPnz8eSJUsQFRWFQ4cOwc7ODl26dMH9+/fNWHnl9fHHH2PZsmX4/PPPcfbsWXz88ceYP38+li5dKvfhmJfP3bt34e/vjy+++KLI7aUZ37CwMJw+fRpxcXGIjY3F3r17MXLkSFN9hEqnpDHPzc3FsWPHEBkZiWPHjiEmJgZJSUno2bOnTj+jjbmgUrt27ZoAIPbs2SOEECIzM1NYWVmJH3/8Ue5z9uxZAUAcOHDAXGUqwp07d4Sfn5+Ii4sTHTt2FBEREUIIjrkxTJkyRbRv377Y7RqNRri5uYlPPvlEbsvMzBTW1tZi3bp1pihRcbp37y5ef/11nbbQ0FARFhYmhOCYGxoAsXnzZvl5acb3zJkzAoBITEyU+2zfvl1IkiTS0tJMVntl9fiYF+Xw4cMCgLh48aIQwrhjzpkdPWRlZQEAXFxcAABHjx5Ffn4+goOD5T6NGjVCnTp1cODAAbPUqBSjR49G9+7ddcYW4Jgbw9atW9GqVSv069cPtWrVQosWLfDVV1/J21NSUpCRkaEz5k5OTmjTpg3HvIyef/55xMfH49y5cwCAkydPYt++fXjllVcAcMyNrTTje+DAATg7O6NVq1Zyn+DgYFhYWODQoUMmr1mJsrKyIEkSnJ2dARh3zHkj0FLSaDQYN24c2rVrh6ZNmwIAMjIyUKVKFfkHVcDV1RUZGRlmqFIZ1q9fj2PHjiExMbHQNo654f37779YtmwZJkyYgP/7v/9DYmIixo4diypVqiA8PFweV1dXV53XcczLburUqcjOzkajRo2gUqmgVqsxZ84chIWFAQDH3MhKM74ZGRmoVauWznZLS0u4uLjwZ2AA9+/fx5QpUzBo0CD5RqDGHHOGnVIaPXo0/vrrL+zbt8/cpShaamoqIiIiEBcXBxsbG3OX81TQaDRo1aoV5s6dCwBo0aIF/vrrL0RFRSE8PNzM1SnThg0bEB0dje+//x7PPPMMTpw4gXHjxsHDw4NjToqXn5+P/v37QwiBZcuWmeQ9eRirFMaMGYPY2Fjs2rULnp6ecrubmxvy8vKQmZmp0//q1atwc3MzcZXKcPToUVy7dg0BAQGwtLSEpaUl9uzZgyVLlsDS0hKurq4ccwNzd3dHkyZNdNoaN26MS5cuAYA8ro+f8cYxL7v33nsPU6dOxcCBA9GsWTO89tprGD9+PObNmweAY25spRlfNzc3XLt2TWf7w4cPcevWLf4MyqEg6Fy8eBFxcXHyrA5g3DFn2CmBEAJjxozB5s2bsXPnTvj6+upsb9myJaysrBAfHy+3JSUl4dKlSwgMDDR1uYoQFBSEU6dO4cSJE/KjVatWCAsLk//OMTesdu3aFbqkwrlz5+Dt7Q0A8PX1hZubm86YZ2dn49ChQxzzMsrNzYWFhe7/flUqFTQaDQCOubGVZnwDAwORmZmJo0ePyn127twJjUaDNm3amLxmJSgIOsnJyfj9999RvXp1ne1GHfNyLW9WuLfffls4OTmJ3bt3i/T0dPmRm5sr9xk1apSoU6eO2Llzpzhy5IgIDAwUgYGBZqxaeR49G0sIjrmhHT58WFhaWoo5c+aI5ORkER0dLapWrSq+++47uc9HH30knJ2dxU8//ST+/PNP0atXL+Hr6yvu3btnxsorr/DwcFG7dm0RGxsrUlJSRExMjKhRo4aYPHmy3IdjXj537twRx48fF8ePHxcAxMKFC8Xx48flM39KM75du3YVLVq0EIcOHRL79u0Tfn5+YtCgQeb6SBVeSWOel5cnevbsKTw9PcWJEyd0vlMfPHgg78NYY86wUwIART5WrVol97l375545513RLVq1UTVqlVF7969RXp6uvmKVqDHww7H3PB+/vln0bRpU2FtbS0aNWokVqxYobNdo9GIyMhI4erqKqytrUVQUJBISkoyU7WVX3Z2toiIiBB16tQRNjY2om7duuL999/X+Z8+x7x8du3aVeT/v8PDw4UQpRvfmzdvikGDBgl7e3vh6Ogohg8fLu7cuWOGT1M5lDTmKSkpxX6n7tq1S96HscZcEuKRS3YSERERKQzX7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERGRojHsEBERkaIx7BAREZGiMewQERmYj48PFi1aZO4yiOi/GHaI6IkkSSrxMWPGDLPWtmXLllL3f+utt6BSqfDjjz8arygiqlAszV0AEVV86enp8t9/+OEHTJ8+Xefmofb29nrtLy8vD1WqVDFYfaWVm5uL9evXY/Lkyfjmm2/Qr18/k9dARKbHmR0ieiI3Nzf54eTkBEmS5Od3795FWFgYXF1dYW9vj9atW+P333/Xeb2Pjw8+/PBDDB06FI6Ojhg5ciQA4KuvvoKXlxeqVq2K3r17Y+HChXB2dtZ57U8//YSAgADY2Nigbt26mDlzJh4+fCjvFwB69+4NSZLk58X58ccf0aRJE0ydOhV79+5FamqqzvZhw4YhJCQEM2fORM2aNeHo6IhRo0YhLy9P7tOpUyeMGTMGY8aMgZOTE2rUqIHIyEiUdOedzMxMvPnmm/I+X3zxRZw8eVLefvLkSXTu3BkODg5wdHREy5YtceTIkRI/CxGVHsMOEZVLTk4OunXrhvj4eBw/fhxdu3ZFjx49cOnSJZ1+n376Kfz9/XH8+HFERkZi//79GDVqFCIiInDixAm89NJLmDNnjs5rEhISMHToUERERODMmTNYvnw5Vq9eLfdLTEwEAKxatQrp6eny8+KsXLkSQ4YMgZOTE1555RWsXr26UJ/4+HicPXsWu3fvxrp16xATE4OZM2fq9FmzZg0sLS1x+PBhLF68GAsXLsTXX39d7Pv269cP165dw/bt23H06FEEBAQgKCgIt27dAgCEhYXB09MTiYmJOHr0KKZOnQorK6sSPwsR6aHctxIloqfKqlWrhJOTU4l9nnnmGbF06VL5ube3twgJCdHpM2DAANG9e3edtrCwMJ19BwUFiblz5+r0+fbbb4W7u7v8HIDYvHnzE+s+d+6csLKyEtevXxdCCLF582bh6+srNBqN3Cc8PFy4uLiIu3fvym3Lli0T9vb2Qq1WCyGE6Nixo2jcuLHO66ZMmSIaN26s83k/++wzIYQQCQkJwtHRUdy/f1+nnnr16only5cLIYRwcHAQq1evfuJnIKKy4cwOEZVLTk4OJk2ahMaNG8PZ2Rn29vY4e/ZsoZmdVq1a6TxPSkrCc889p9P2+POTJ09i1qxZsLe3lx8jRoxAeno6cnNz9arzm2++QZcuXVCjRg0AQLdu3ZCVlYWdO3fq9PP390fVqlXl54GBgcjJydE55NW2bVtIkqTTJzk5GWq1utD7njx5Ejk5OahevbrO50hJScH58+cBABMmTMCbb76J4OBgfPTRR3I7ERkGFygTUblMmjQJcXFx+PTTT1G/fn3Y2tqib9++OutcAMDOzk7vfefk5GDmzJkIDQ0ttM3GxqbU+1Gr1VizZg0yMjJgaWmp0/7NN98gKChI79pKKycnB+7u7ti9e3ehbQXrk2bMmIHBgwfjl19+wfbt2/HBBx9g/fr16N27t9HqInqaMOwQUbns378fw4YNk7+Yc3JycOHChSe+rmHDhoXW2Dz+PCAgAElJSahfv36x+7GysipyRuVR27Ztw507d3D8+HGoVCq5/a+//sLw4cORmZkpB4+TJ0/i3r17sLW1BQAcPHgQ9vb28PLykl936NAhnf0fPHgQfn5+Ovt+9DMUhKySFlA3aNAADRo0wPjx4zFo0CCsWrWKYYfIQHgYi4jKxc/PDzExMThx4gROnjyJwYMHQ6PRPPF17777LrZt24aFCxciOTkZy5cvx/bt23UOD02fPh1r167FzJkzcfr0aZw9exbr16/Hf/7zH7mPj48P4uPjkZGRgdu3bxf5XitXrkT37t3h7++Ppk2byo/+/fvD2dkZ0dHRct+8vDy88cYbOHPmDLZt24YPPvgAY8aMgYXF//53eenSJUyYMAFJSUlYt24dli5dioiIiCLfOzg4GIGBgQgJCcFvv/2GCxcu4I8//sD777+PI0eO4N69exgzZgx2796NixcvYv/+/UhMTETjxo2fOIZEVDoMO0RULgsXLkS1atXw/PPPo0ePHujSpQsCAgKe+Lp27dohKioKCxcuhL+/P3799VeMHz9e5/BUly5dEBsbi99++w2tW7dG27Zt8dlnn8Hb21vus2DBAsTFxcHLywstWrQo9D5Xr17FL7/8gj59+hTaZmFhgd69e2PlypVyW1BQEPz8/PDCCy9gwIAB6NmzZ6GLJg4dOhT37t3Dc889h9GjRyMiIkI+nf5xkiRh27ZteOGFFzB8+HA0aNAAAwcOxMWLF+Hq6gqVSoWbN29i6NChaNCgAfr3749XXnml0BlgRFR2khAlXByCiMiERowYgb///hsJCQlmef9hw4YhMzOzxCsyd+rUCc2bN+ftIIgqEa7ZISKz+fTTT/HSSy/Bzs4O27dvx5o1a/Dll1+auywiUhiGHSIym8OHD2P+/Pm4c+cO6tatiyVLluDNN980d1lEpDA8jEVERESKxgXKREREpGgMO0RERKRoDDtERESkaAw7REREpGgMO0RERKRoDDtERESkaAw7REREpGgMO0RERKRoDDtERESkaP8PZ3QtHzm9aqIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert tensors to numpy for plotting\n",
    "predicted_np = preds.detach().numpy()\n",
    "targets_np = targets.detach().numpy()\n",
    "\n",
    "plt.scatter(targets_np[:, 0], targets_np[:, 1], color='blue', label='Actual')\n",
    "plt.scatter(predicted_np[:, 0], predicted_np[:, 1], color='red', label='Predicted')\n",
    "plt.xlabel('Target Apples')\n",
    "plt.ylabel('Target Oranges')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cf707-2051-43a9-9ff7-a056910561c2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we explored the fundamentals of linear regression, a powerful tool for predictive modeling. We learned how to implement a linear regression model from scratch using PyTorch, optimize it with gradient descent, and evaluate its performance through visualizations and metrics.\n",
    "\n",
    "While our model provides a solid introduction to linear regression, there are several areas for improvement that I plan to explore in future projects:\n",
    "\n",
    "1. **Feature Engineering:** I will experiment with polynomial features and feature scaling to capture more complex relationships in the data.\n",
    "\n",
    "2. **Model Complexity:** I aim to implement more advanced models, such as neural networks, to enhance predictive performance.\n",
    "\n",
    "3. **Hyperparameter Tuning:** I will utilize techniques like Grid Search or Random Search to optimize hyperparameters for better model performance.\n",
    "\n",
    "4. **Regularization Techniques:** Implementing L1 and L2 regularization will help prevent overfitting and improve generalization.\n",
    "\n",
    "5. **Cross-Validation:** I plan to use K-Fold cross-validation to ensure that my model generalizes well to unseen data.\n",
    "\n",
    "6. **Advanced Evaluation Metrics:** I will incorporate multiple evaluation metrics, such as R-squared and Mean Absolute Error, to provide a comprehensive assessment of model performance.\n",
    "\n",
    "7. **Visualization of Results:** Creating residual plots will help visualize prediction errors and assess model performance more effectively.\n",
    "\n",
    "8. **Documentation and Presentation:** I will focus on clear documentation and engaging presentations to make my findings accessible to a broader audience.\n",
    "\n",
    "I hope this guide inspires you to dive deeper into the world of machine learning. Stay tuned for my next project, where I will implement these improvements and share my findings. Happy coding, and feel free to reach out if you have any questions or want to share your own experiences with linear regression!\n",
    "\n",
    "---\n",
    "\n",
    "### Rating of the Model\n",
    "\n",
    "On a scale from 1 to 10, I would rate this model an **8**. \n",
    "\n",
    "#### Rationale:\n",
    "- **Strengths:**\n",
    "  - Successfully implements linear regression using PyTorch.\n",
    "  - Provides a good foundation for beginners to understand the concepts of linear regression and optimization.\n",
    "  - Includes visualizations to assess model performance.\n",
    "  - Outlines future directions for improvement, demonstrating a commitment to continuous learning.\n",
    "\n",
    "- **Areas for Improvement:**\n",
    "  - Exploring more complex models and techniques.\n",
    "  - Enhancing feature engineering and hyperparameter tuning.\n",
    "  - Utilizing cross-validation and advanced evaluation metrics for a more comprehensive assessment.\n",
    "\n",
    "This rating reflects the model's educational value and its potential for improvement as I continue to learn and experiment in the field of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598af33-f1e0-4cf9-a5d2-03bdf7e1945e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
